#!/usr/bin/env python3
"""
wan2_workflow_v2.py

Pure-Python WAN2.2-style workflow (ComfyUI-free).

Goals & features implemented here (framework-level):
 - memory-aware model loading/unloading (CPU offload where possible)
 - pluggable model-loaders (WAN SDK / custom loaders) via clear hooks
 - IP-Adapter (identity) implementation using CLIP vision encoder
 - Control-adapter hooks (pose/depth/edge) placeholders and stubs
 - Latent cache to disk for non-destructive editing
 - Rich refine pipeline: camera, lighting, temporal/physics corrections
 - Scene-JSON runner that produces per-scene latents & videos and compiles final
 - CLI + simple programmatic API

IMPORTANT:
 - This is implementation-level glue code. Replace the placeholders in
   `WANEngine._load_unet/_load_vae/_sample` with your real WAN2.2 model calls
   or adapt the provided safetensors-loading helpers if you have the
   model architectures available in Python.
 - The IP-Adapter here uses CLIP (from transformers) as a light-weight identity
   encoder â€” it is not the original trained IP-Adapter network, but it provides
   strong identity/style embeddings for consistency across scenes.

"""

from __future__ import annotations

import argparse
import json
import os
import random
import shutil
import subprocess
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from PIL import Image

import torch
import torch.nn.functional as F
from transformers import CLIPImageProcessor, CLIPVisionModel

# Optional: safetensors helper if you want to inspect weights
try:
    from safetensors.torch import load_file as safetensor_load
    SAFETENSORS_AVAILABLE = True
except Exception:
    SAFETENSORS_AVAILABLE = False

# -------------------------
# Basic configuration
# -------------------------
BASE = Path(__file__).parent
DEFAULT_MODELS_DIR = BASE / "models"
CACHE_DIR = BASE / "cache_latents"
ASSETS_DIR = BASE / "assets"

os.makedirs(DEFAULT_MODELS_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(ASSETS_DIR, exist_ok=True)

DEFAULT_CONFIG = {
    "fp_precision": "fp16",
    "default_fps": 16,
    "default_frames": 64,
    "default_width": 512,
    "default_height": 512,
}

# -------------------------
# Utilities
# -------------------------

def printt(*args, **kwargs):
    print("[wan2flow]", *args, **kwargs)


def detect_device(preferred: Optional[str] = None) -> str:
    """Return 'cuda' or 'cpu' depending on availability and user preference."""
    if preferred:
        if preferred.lower() == "cpu":
            return "cpu"
        if preferred.lower() in ("cuda", "gpu", "a40", "4090"):
            return "cuda" if torch.cuda.is_available() else "cpu"
    return "cuda" if torch.cuda.is_available() else "cpu"

# -------------------------
# IP-Adapter (CLIP-based identity encoder)
# -------------------------

class IPAdapter:
    """Lightweight IP-Adapter like helper using CLIP vision encoder.

    Purpose: produce a stable identity/style embedding per reference image and
    supply that embedding to the diffusion UNet via cross-attention injection
    or as concatenated conditioning vectors.
    """

    def __init__(self, device: str = "cpu"):
        self.device = device
        self._model = None
        self._processor = None

    def _ensure_model(self):
        if self._model is None:
            printt("Loading CLIP vision model for IP-Adapter (this is lightweight)...")
            self._processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
            self._model = CLIPVisionModel.from_pretrained("openai/clip-vit-large-patch14").to(self.device)
            self._model.eval()

    def compute_embedding(self, image_path: str, resize: Tuple[int,int]=(224,224)) -> torch.Tensor:
        self._ensure_model()
        img = Image.open(image_path).convert("RGB")
        proc = self._processor(images=img, return_tensors="pt")
        # move to device
        for k,v in proc.items():
            proc[k] = v.to(self.device)
        with torch.no_grad():
            out = self._model(**proc).pooler_output
        emb = out / (out.norm(dim=-1, keepdim=True) + 1e-8)
        return emb.detach()

    def batch_compute(self, image_paths: List[str]) -> torch.Tensor:
        embs = [self.compute_embedding(p) for p in image_paths]
        return torch.cat(embs, dim=0)

# -------------------------
# Control Adapter stubs (pose/depth/edge)
# -------------------------

def detect_pose_from_image(image_path: str) -> Dict[str, Any]:
    """Placeholder: return a pose map or keypoints for a given image.

    In production, integrate MediaPipe/OpenPose/Detectron to extract skeletons.
    Return a small dict with either 'keypoints' or a pose image path.
    """
    # Minimal stub
    return {"pose_image": None, "keypoints": None}

def detect_depth_from_image(image_path: str) -> str:
    """Placeholder: return a path to a depth map (PNG) or depth tensor.
    In production, run a MiDaS or similar depth estimator and return file.
    """
    return None

# -------------------------
# WAN engine abstraction (no ComfyUI)
# -------------------------

@dataclass
class WANEngine:
    models_dir: Path = DEFAULT_MODELS_DIR
    device: str = "cuda"
    precision: str = "fp16"
    low_vram: bool = False
    models: Dict[str, Any] = None

    def __post_init__(self):
        self.models = {}

    # ---- model loading hooks (replace with real WAN SDK or model classes) ----
    def load_unet(self, path: str, variant: str = "high"):
        """Load UNet weights. Replace with actual model class loading.
        Example: unet = WANUNet.from_safetensors(path); unet.to(device)
        """
        printt(f"[WANEngine] (placeholder) load_unet {path} variant={variant} to {self.device}")
        # Placeholder store
        self.models[f"unet_{variant}"] = {"path": path, "variant": variant}

    def load_vae(self, path: str):
        printt(f"[WANEngine] (placeholder) load_vae {path} to {self.device}")
        self.models["vae"] = {"path": path}

    def load_text_encoder(self, path: str):
        printt(f"[WANEngine] (placeholder) load_text_encoder {path} to {self.device}")
        self.models["text_encoder"] = {"path": path}

    def unload_models(self):
        printt("[WANEngine] unloading models (placeholders)")
        self.models = {}
        torch.cuda.empty_cache()

    # ---- generation hook (replace with WAN sampling code) ----
    def sample_latent(self,
                      prompt: str,
                      num_frames: int = 64,
                      height: int = 512,
                      width: int = 512,
                      guidance_scale: float = 7.5,
                      motion_strength: float = 1.0,
                      noise_ratio: float = 0.5,
                      steps: int = 20,
                      seed: Optional[int] = None,
                      ip_adapter_emb: Optional[torch.Tensor] = None,
                      control_hint: Optional[Dict[str, Any]] = None) -> torch.Tensor:
        """
        Placeholder sampler: returns a random latent shaped (T, 4, H/8, W/8).

        Replace this with calls into your UNet/VAE sampling loop. The signature
        mirrors the expectations of the rest of this file.
        """
        dtype = torch.float16 if (self.precision == "fp16" and self.device == "cuda") else torch.float32
        h8, w8 = max(1, height // 8), max(1, width // 8)
        torch.manual_seed(seed or random.randint(0, 2**31 - 1))
        latent = torch.randn((num_frames, 4, h8, w8), dtype=dtype, device="cpu")
        printt(f"[WANEngine] (placeholder) sample_latent -> {latent.shape} on cpu")
        return latent

    def decode_latent(self, latent: torch.Tensor) -> List[Image.Image]:
        """Decode latent to list of PIL images using your VAE/decoder.

        Placeholder: simple grayscale mapping.
        """
        latent = latent.cpu()
        frames = []
        arr = latent.numpy()
        for i in range(arr.shape[0]):
            chan = arr[i, 0]
            img = np.clip((chan - chan.min()) / (chan.ptp() + 1e-8) * 255, 0, 255).astype(np.uint8)
            pil = Image.fromarray(np.stack([img, img, img], axis=-1))
            frames.append(pil)
        return frames

# -------------------------
# Latent cache utilities
# -------------------------

def save_latent(latent: torch.Tensor, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(latent.cpu(), path)
    printt("Saved latent:", path)


def load_latent(path: Path) -> torch.Tensor:
    return torch.load(path)

# -------------------------
# Refinement pipeline
# -------------------------

def refine_latent(latent: torch.Tensor,
                  feedback_prompt: str = "",
                  strength: float = 0.35,
                  camera_hint: Optional[str] = None,
                  lighting_hint: Optional[str] = None) -> torch.Tensor:
    """Apply edits to a cached latent based on feedback.

    Strategies implemented:
      - temporal smoothing (kernel averaging)
      - exposure normalization (per-frame mean balancing)
      - small spatial transforms for camera-like effects
      - motion amplification/damping

    This operates purely in latent space for speed.
    """
    latent = latent.clone()
    dtype = latent.dtype
    if dtype == torch.float16:
        latent = latent.float()

    p = (feedback_prompt or "").lower()

    # kernel size
    kernel = 3
    if "smooth" in p or "gentle" in p:
        kernel = 5
    if "ultra smooth" in p or "very smooth" in p:
        kernel = 7

    pad = kernel // 2
    latent_t = latent.unsqueeze(0) if latent.dim() == 3 else latent
    # average pool along time
    smoothed = F.avg_pool3d(latent_t.unsqueeze(0), kernel_size=(kernel, 1, 1), stride=1, padding=(pad, 0, 0)).squeeze(0)
    latent = (1 - strength) * latent + strength * smoothed

    # exposure balance
    mean_per_frame = latent.mean(dim=[1, 2, 3], keepdim=True)
    global_mean = mean_per_frame.mean()
    latent = latent + (global_mean - mean_per_frame) * 0.3

    # camera micro transforms based on camera_hint or prompt
    if camera_hint and "pan left" in camera_hint:
        latent = torch.roll(latent, shifts=-2, dims=-1)
    if camera_hint and "pan right" in camera_hint:
        latent = torch.roll(latent, shifts=2, dims=-1)

    if "zoom in" in p:
        latent = latent * 1.02
    if "zoom out" in p:
        latent = latent * 0.98

    # clamp to safe numeric ranges
    latent = latent.clamp(-10.0, 10.0)
    if dtype == torch.float16:
        latent = latent.half()
    return latent

# -------------------------
# High-level scene runner
# -------------------------

def render_scene(engine: WANEngine,
                 prompt: str,
                 out_base: str,
                 frames: int = DEFAULT_CONFIG["default_frames"],
                 width: int = DEFAULT_CONFIG["default_width"],
                 height: int = DEFAULT_CONFIG["default_height"],
                 guidance: float = 7.5,
                 motion: float = 1.0,
                 noise_ratio: float = 0.5,
                 steps: int = 20,
                 seed: Optional[int] = None,
                 ip_emb: Optional[torch.Tensor] = None,
                 control_hint: Optional[Dict[str,Any]] = None) -> Dict[str, Any]:
    """Full per-scene flow: sample latent -> save -> refine auto -> decode -> save mp4

    Returns dict with paths to latent, refined latent, and mp4 outputs.
    """
    out_base = Path(out_base)
    out_base.parent.mkdir(parents=True, exist_ok=True)

    # 1. sample latent
    latent = engine.sample_latent(
        prompt=prompt,
        num_frames=frames,
        height=height,
        width=width,
        guidance_scale=guidance,
        motion_strength=motion,
        noise_ratio=noise_ratio,
        steps=steps,
        seed=seed,
        ip_adapter_emb=ip_emb,
        control_hint=control_hint,
    )
    latent_path = CACHE_DIR / f"{out_base.name}.latent.pt"
    save_latent(latent, latent_path)

    # 2. automatic light/physics refine
    refined = refine_latent(latent, feedback_prompt="", strength=0.28)
    refined_path = CACHE_DIR / f"{out_base.name}.refined.latent.pt"
    save_latent(refined, refined_path)

    # 3. decode
    frames_pil = engine.decode_latent(refined)
    # save frames and mux to mp4 via ffmpeg
    tmpd = tempfile.mkdtemp(prefix=f"wan_decode_{out_base.name}_")
    for i, pil in enumerate(frames_pil):
        pil.save(os.path.join(tmpd, f"frame_{i:04d}.png"))

    mp4_path = str(out_base.with_suffix('.final.mp4'))
    cmd = [
        "ffmpeg", "-y", "-loglevel", "error",
        "-framerate", str(DEFAULT_CONFIG["default_fps"]),
        "-i", os.path.join(tmpd, "frame_%04d.png"),
        "-c:v", "libx264", "-pix_fmt", "yuv420p", mp4_path
    ]
    printt("Running ffmpeg to produce mp4 for", out_base.name)
    subprocess.check_call(cmd)
    shutil.rmtree(tmpd)

    return {"latent_path": str(latent_path), "refined_path": str(refined_path), "final_mp4": mp4_path}

# -------------------------
# Scene JSON driver
# -------------------------

def render_script(engine: WANEngine, script: Dict[str, Any], output_dir: str = "outputs", ip_adapter: Optional[IPAdapter]=None, device: str = 'cuda') -> Dict[str, Any]:
    """Render scenes described in script JSON (format documented in UI). Returns list of results."""
    out_dir = Path(output_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    results = []

    # optionally precompute identity embeddings
    char_embs: Dict[str, torch.Tensor] = {}
    if ip_adapter:
        # look for global characters list
        chars = script.get('characters', None)
        if chars:
            for c in chars:
                imgs = c.get('images', [])
                if imgs:
                    char_embs[c['name']] = ip_adapter.batch_compute(imgs)

    for scene in script.get('scenes', []):
        sid = scene.get('id', f"scene_{random.randint(0,9999)}")
        summary = scene.get('summary', '')
        camera = scene.get('camera', '')
        duration = float(scene.get('duration', 6.0))
        frames = int(DEFAULT_CONFIG['default_fps'] * duration)

        # build prompt merging summary + camera + style tokens
        prompt = f"{summary}. Camera: {camera}."
        out_base = Path(output_dir) / sid

        # choose IP embedding if characters referenced
        ip_emb = None
        char_ref = scene.get('character', None)
        if char_ref and char_ref in char_embs:
            ip_emb = char_embs[char_ref]

        printt(f"Rendering scene {sid}: frames={frames} prompt={prompt}")
        res = render_scene(engine, prompt=prompt, out_base=str(out_base), frames=frames, ip_emb=ip_emb)
        results.append({"scene_id": sid, **res})

    # optional: combine all mp4s
    if results:
        clips_list = [r['final_mp4'] for r in results]
        # use ffmpeg concat
        concat_txt = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt')
        for p in clips_list:
            concat_txt.write(f"file '{os.path.abspath(p)}'\n")
        concat_txt.close()
        final_comp = Path(output_dir) / "final_compilation.mp4"
        cmd = ["ffmpeg", "-y", "-loglevel", "error", "-f", "concat", "-safe", "0", "-i", concat_txt.name, "-c", "copy", str(final_comp)]
        printt("Concatenating scenes into final compilation...")
        subprocess.check_call(cmd)
        os.unlink(concat_txt.name)
        return {"scenes": results, "final_compilation": str(final_comp)}

    return {"scenes": results}

# -------------------------
# CLI / programmatic API
# -------------------------

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--mode", choices=["generate", "render_script", "refine"], required=True)
    p.add_argument("--prompt", type=str, default="")
    p.add_argument("--out", type=str, required=True)
    p.add_argument("--latent", type=str, default="")
    p.add_argument("--frames", type=int, default=None)
    p.add_argument("--width", type=int, default=None)
    p.add_argument("--height", type=int, default=None)
    p.add_argument("--seed", type=int, default=None)
    p.add_argument("--device", type=str, default=None)
    p.add_argument("--precision", type=str, choices=["fp32","fp16"], default="fp16")
    p.add_argument("--models_dir", type=str, default=str(DEFAULT_MODELS_DIR))
    p.add_argument("--script_json", type=str, default="")
    p.add_argument("--feedback", type=str, default="")
    return p.parse_args()


def main():
    args = parse_args()
    device = detect_device(args.device)
    engine = WANEngine(models_dir=Path(args.models_dir), device=device, precision=args.precision)

    if args.mode == 'generate':
        frames = args.frames or DEFAULT_CONFIG['default_frames']
        width = args.width or DEFAULT_CONFIG['default_width']
        height = args.height or DEFAULT_CONFIG['default_height']
        res = render_scene(engine, prompt=args.prompt, out_base=args.out, frames=frames, width=width, height=height, seed=args.seed)
        printt(json.dumps(res, indent=2))

    elif args.mode == 'refine':
        if not args.latent:
            raise RuntimeError("--latent required for refine mode")
        latent = load_latent(Path(args.latent))
        refined = refine_latent(latent, feedback_prompt=args.feedback)
        out_path = Path(args.out)
        save_latent(refined, Path(CACHE_DIR) / f"{out_path.name}.refined.latent.pt")
        # decode & save simple mp4
        frames_pil = engine.decode_latent(refined)
        tmpd = tempfile.mkdtemp(prefix=f"wan_refine_{out_path.name}_")
        for i, pil in enumerate(frames_pil):
            pil.save(os.path.join(tmpd, f"frame_{i:04d}.png"))
        mp4_path = str(out_path.with_suffix('.final.mp4'))
        cmd = ["ffmpeg","-y","-loglevel","error","-framerate",str(DEFAULT_CONFIG['default_fps']),"-i",os.path.join(tmpd,"frame_%04d.png"),"-c:v","libx264","-pix_fmt","yuv420p",mp4_path]
        subprocess.check_call(cmd)
        shutil.rmtree(tmpd)
        printt({"refined_mp4": mp4_path})

    elif args.mode == 'render_script':
        if not args.script_json:
            raise RuntimeError("--script_json path required for render_script mode")
        with open(args.script_json, 'r') as f:
            script = json.load(f)
        ip = IPAdapter(device=device)
        res = render_script(engine, script, output_dir=args.out, ip_adapter=ip)
        printt(json.dumps(res, indent=2))

if __name__ == '__main__':
    main()
